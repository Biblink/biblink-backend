{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Scripts for GloVe Embeddings\n",
    "\n",
    "Author: Brandon Fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bible_data = json.load(open('../bible-files/english-web-bible.json', encoding='utf-8-sig'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verse_data = []\n",
    "for book in bible_data:\n",
    "    for chapter in book['data']:\n",
    "        for verses in chapter['verses']:\n",
    "           verse_data.append(verses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "def tokenize_data(verse_data):\n",
    "    for verse in verse_data:\n",
    "        text = ''.join(ch for ch in verse['text'] if ch not in exclude)\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        final_text = []\n",
    "        for val in tokenized_text:\n",
    "            if val not in stopwords:\n",
    "                if 'Yahweh' in val:\n",
    "                    val = val.replace('Yahweh', 'God')\n",
    "                final_text.append(val)\n",
    "        verse['tokenized_text'] = final_text\n",
    "tokenize_data(verse_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_data_file = '../server/files/glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 813 ms, total: 27.2 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_data = {}\n",
    "with open(glove_data_file) as f:\n",
    "    for line in list(f.readlines()):\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        data = np.array([float(i) for i in split[1:]])\n",
    "        glove_data[word] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(w):\n",
    "  return glove_data[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec('okay').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Dictionary: 20.971616 mb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Size of Dictionary: ' + str(sys.getsizeof(glove_data) / 1000000) + ' mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Words to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31103"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_data_with_glove = []\n",
    "for verse in verse_data:\n",
    "    vector = np.array([])\n",
    "    for word in verse['tokenized_text'][:MAX_LEN]:\n",
    "        try:\n",
    "            vector = np.append(vector, vec(word))\n",
    "        except Exception:\n",
    "            vector = np.append(vector, np.zeros(200))\n",
    "    if vector.shape[0] < MAX_LEN * 200:\n",
    "        vector = np.append(vector, np.zeros([MAX_LEN * 200 - vector.shape[0]]))\n",
    "    verse['vector'] = vector\n",
    "    verse_data_with_glove.append(verse)\n",
    "len(verse_data_with_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_data_with_glove[0]['vector'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sklearn.preprocessing as pp\n",
    "from scipy import sparse\n",
    "verse_sparse = sparse.coo_matrix([verse['vector'] for verse in verse_data_with_glove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cos_sim_matrix = cosine_similarity(verse_sparse, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'For I know that Yahweh is great,\\nthat our Lord is above all gods.',\n",
       " 'tokenized_text': ['For', 'I', 'know', 'God', 'great', 'Lord', 'gods'],\n",
       " 'vector': array([ 0.,  0.,  0., ...,  0.,  0.,  0.]),\n",
       " 'verse': 'Psalms 135:5',\n",
       " 'verse_number': '5'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_data[np.argmax(cos_sim_matrix[0][1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7739.172984"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(cos_sim_matrix) / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Similar Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar_values(verse, total_values=10):\n",
    "    proper_index = None\n",
    "    for index, verse_text in enumerate(verse_data):\n",
    "        if verse_text['verse'] == verse:\n",
    "            proper_index = index\n",
    "            break\n",
    "    sim_text = cos_sim_matrix[proper_index][1:]\n",
    "    final_text = []\n",
    "    final_indices = list(reversed(np.argsort(sim_text)))[:total_values]\n",
    "    for index in final_indices:\n",
    "        final_text.append(verse_data[index])\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psalms 135:5: For I know that Yahweh is great,\n",
      "that our Lord is above all gods.\n",
      "\n",
      "\n",
      "Psalms 33:13: Yahweh looks from heaven.\n",
      "He sees all the sons of men.\n",
      "\n",
      "\n",
      "Psalms 105:6: you offspring of Abraham, his servant,\n",
      "you children of Jacob, his chosen ones.\n",
      "\n",
      "\n",
      "1 Chronicles 16:13: you offspring of Israel his servant,\n",
      "you children of Jacob, his chosen ones.\n",
      "\n",
      "\n",
      "Job 8:8: “Please inquire of past generations.\n",
      "Find out about the learning of their fathers.\n",
      "\n",
      "\n",
      "John 1:1: In the beginning was the Word, and the Word was with God, and the Word was God.\n",
      "\n",
      "\n",
      "Luke 24:53: and were continually in the temple, praising and blessing God. Amen.\n",
      "\n",
      "\n",
      "Matthew 28:20: teaching them to observe all things that I commanded you. Behold, I am with you always, even to the end of the age.” Amen.\n",
      "\n",
      "\n",
      "Psalms 34:15: Yahweh’s eyes are toward the righteous.\n",
      "His ears listen to their cry.\n",
      "\n",
      "\n",
      "Genesis 9:18: The sons of Noah who went out from the ship were Shem, Ham, and Japheth. Ham is the father of Canaan.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_values = get_similar_values('Genesis 1:1', total_values=10)\n",
    "for value in similar_values:\n",
    "    print(value['verse'] + ': ' + value['text'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sim_lookup.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(cos_sim_matrix, 'sim_lookup.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning GloVe for Bible Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brandon Fan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(glove_data_file, encoding='utf8')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_write = open('glove_gensim.model', 'w', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimension_string = str(len(lines)) + ' ' + glove_data_file.split('.')[-2].split('d')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_write.write(dimension_string)\n",
    "f_write.write('\\n')\n",
    "for i in lines:\n",
    "     f_write.write(i)\n",
    "f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2vec_keys = []\n",
    "with open( 'glove_gensim.model', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        # word and vec\n",
    "        Word2vec_keys.append( line[0] )\n",
    "\n",
    "tokenizer = RegexpTokenizer( r\"\\w+|[^\\w\\s]\" ) # include punctunation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec( [ Word2vec_keys ], size=200, alpha=0.0001, workers=10, \n",
    "                               min_alpha=1e-6, iter=55, min_count=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = [verse['tokenized_text'] for verse in verse_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build_vocab(corpora, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18653597"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train( corpora, total_examples = model.corpus_count, epochs = model.iter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'glove_gensim_finetuned.txt', 'w+', encoding='utf8' ) as F:\n",
    "    for key in model.wv.vocab.keys():\n",
    "        vec = model[ key ]\n",
    "        vec = [ str( float( item ) ) for item in list( vec ) ]\n",
    "        line = key + ' ' + ' '.join( list( vec ) ) + '\\n'\n",
    "        F.write( line )\n",
    "    F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetuned_words = pd.read_table('glove_gensim_finetuned.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_finetuned(w):\n",
    "  return finetuned_words.loc[w].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31103"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_data_with_glove_finetuned = []\n",
    "for verse in verse_data:\n",
    "    vector = np.array([])\n",
    "    for word in verse['tokenized_text'][:MAX_LEN]:\n",
    "        try:\n",
    "            vector = np.append(vector, vec(word))\n",
    "        except Exception:\n",
    "            vector = np.append(vector, np.zeros(200))\n",
    "    if vector.shape[0] < MAX_LEN * 200:\n",
    "        vector = np.append(vector, np.zeros([MAX_LEN * 200 - vector.shape[0]]))\n",
    "    verse['vector'] = vector\n",
    "    verse_data_with_glove_finetuned.append(verse)\n",
    "len(verse_data_with_glove_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_sim_matrix_finetuned = cosine_similarity([verse['vector'] for verse in verse_data_with_glove_finetuned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sim_lookup.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(cos_sim_matrix_finetuned, 'sim_lookup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar_values_finetuned(verse, total_values=10):\n",
    "    proper_index = None\n",
    "    for index, verse_text in enumerate(verse_data):\n",
    "        if verse_text['verse'] == verse:\n",
    "            proper_index = index\n",
    "            break\n",
    "    sim_text = cos_sim_matrix_finetuned[proper_index][1:]\n",
    "    final_text = []\n",
    "    final_indices = list(reversed(np.argsort(sim_text)))[:total_values]\n",
    "    for index in final_indices:\n",
    "        final_text.append(verse_data[index])\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revelation 22:20: He who testifies these things says,\n",
      "“Yes, I come quickly.”\n",
      "\n",
      "Amen! Yes, come, Lord Jesus.\n",
      "\n",
      "\n",
      "1 Chronicles 4:1: The sons of Judah: Perez, Hezron, Carmi, Hur, and Shobal.\n",
      "\n",
      "\n",
      "1 Chronicles 2:52: Shobal the father of Kiriath Jearim had sons: Haroeh, half of the Menuhoth.\n",
      "\n",
      "\n",
      "1 Chronicles 2:53: The families of Kiriath Jearim: the Ithrites, the Puthites, the Shumathites, and the Mishraites; from them came the Zorathites and the Eshtaolites.\n",
      "\n",
      "\n",
      "1 Chronicles 2:54: The sons of Salma: Bethlehem, the Netophathites, Atroth Beth Joab, and half of the Manahathites, the Zorites.\n",
      "\n",
      "\n",
      "1 Chronicles 2:55: The families of scribes who lived at Jabez: the Tirathites, the Shimeathites, and the Sucathites. These are the Kenites who came from Hammath, the father of the house of Rechab.\n",
      "\n",
      "\n",
      "1 Chronicles 3:1: Now these were the sons of David, who were born to him in Hebron: the firstborn, Amnon, of Ahinoam the Jezreelitess; the second, Daniel, of Abigail the Carmelitess;\n",
      "\n",
      "\n",
      "1 Chronicles 3:2: the third, Absalom the son of Maacah the daughter of Talmai king of Geshur; the fourth, Adonijah the son of Haggith;\n",
      "\n",
      "\n",
      "1 Chronicles 3:3: the fifth, Shephatiah of Abital; the sixth, Ithream by Eglah his wife:\n",
      "\n",
      "\n",
      "1 Chronicles 3:4: six were born to him in Hebron; and he reigned there seven years and six months. He reigned thirty-three years in Jerusalem;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_values = get_similar_values_finetuned('Genesis 1:1', total_values=10)\n",
    "for value in similar_values:\n",
    "    print(value['verse'] + ': ' + value['text'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
