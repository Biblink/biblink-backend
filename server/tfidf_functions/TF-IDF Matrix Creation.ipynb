{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Matrix Creation\n",
    "\n",
    "Authors: Brandon Fan, Jordan Seiler\n",
    "Last Edit Date: 11/29/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brandonfan1256/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brandonfan1256/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common imports\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# important imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# nltk imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# install necessary nltk corpuses as needed\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Bible data\n",
    "BIBLE = json.load(open('../../bible-files/english-web-bible.json', 'r', encoding='utf-8-sig'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create corpus of data\n",
    "corpus = []\n",
    "for name in bible:\n",
    "    chapters = name[\"data\"]\n",
    "    for chapter in chapters:\n",
    "        verses = chapter[\"verses\"]\n",
    "        for verse in verses:\n",
    "            text = verse[\"text\"]\n",
    "            corpus.append({'ref': verse['verse'], 'tokenized': word_tokenize(text)}) \n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make punctuation table for substitution\n",
    "PUNCT_TABLE = str.maketrans({key: None for key in string.punctuation})\n",
    "# get stopwords from nltk\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "# create regex pattern to get rid of unicode hyphen and quotations\n",
    "PATTERN = r'[\\â€”\\'\\\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entry in corpus:\n",
    "    # gets tokenized words\n",
    "    words = entry['tokenized']\n",
    "    # remove hyphens, quotes and punctuations from each word\n",
    "    tokenized_cp = [re.sub(PATTERN, '', word.translate(PUNCT_TABLE)) for word in words if word not in STOP_WORDS]\n",
    "    # remove all non-unicode/alphanumeric characters from each word \n",
    "    # and only get words that have a length greater than 1\n",
    "    tokenized_cp = np.array([''.join(e for e in word if e.isalnum()) for word in tokenized_cp if len(word) > 1])\n",
    "    entry['tokenized'] = tokenized_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate term frequencies per document we follow the formula:\n",
    "\n",
    "$$TF(t,d \\in D) = \\frac{\\text{Number of times term } t \\text{ appears in document }d}{\\text{Total number of terms in document } d}$$\n",
    "\n",
    "Where $D$ is the corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for entry in corpus:\n",
    "    words = entry['tokenized']\n",
    "    # get unique words and their counts per document\n",
    "    unique_words_and_counts = np.unique(words, return_counts=True)\n",
    "    unique_words = unique_words_and_counts[0]\n",
    "    counts = unique_words_and_counts[1]\n",
    "    # set dictionary values of such\n",
    "    entry['unique_words'] = unique_words\n",
    "    entry['term_frequency'] = {}\n",
    "    # calculate term frequencies for each word\n",
    "    # TF = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "    # using numpy matrix division to calculate TF\n",
    "    for index, value in enumerate(counts / len(words)):\n",
    "        entry['term_frequency'][unique_words[index]] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Inverse Document Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate inverse document frequencies we follow the formula:\n",
    "\n",
    "$$ IDF(t, D) = \\log_e\\left(\\frac{\\text{Total number of documents } D}{\\text{Number of documents with term } t \\text{ in it}}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all unique words per document and compile into list\n",
    "unique_words = []\n",
    "for entry in corpus:\n",
    "    unique_words += entry['unique_words'].tolist()\n",
    "unique_words = np.array(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all unique words of all the documents and get their counts\n",
    "unique_of_unique_words = np.unique(unique_words, return_counts=True)\n",
    "words_list = unique_of_unique_words[0]\n",
    "# calculate inverse document frequencies\n",
    "# IDF = ln(Total number of documents / Number of documents with term t in it)\n",
    "# using numpy matrix division and log to calculate IDF\n",
    "idf = np.log(len(corpus) / unique_of_unique_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct idf weights dictionary\n",
    "idf_dict = {}\n",
    "for i in range(len(idf)):\n",
    "    idf_dict[words_list[i]] = idf[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF Weights Per Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the TF-IDF weights per document we do the following:\n",
    "\n",
    "$$ W(t, d \\in D) = TF(t, d) \\times IDF(t, D) $$\n",
    "\n",
    "Where $W$ is the TF-IDF weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate TF-IDF Weights\n",
    "for entry in corpus:\n",
    "    # initialize dictionary to hold weights per document\n",
    "    entry['weights'] = {}\n",
    "    for word in entry['term_frequency'].keys():\n",
    "        # calculate weight for each word per document\n",
    "        # TF * IDF = weight of term t per document\n",
    "        entry['weights'][word] = entry['term_frequency'][word] * idf_dict[word] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index For Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates dictionary of indices\n",
    "indices = {}\n",
    "for entry in corpus:\n",
    "    # iterate through all of the weights\n",
    "    for term in entry['weights'].keys():\n",
    "        # if term exists already in dictionary then just append\n",
    "        if term in indices.keys():\n",
    "            indices[term].append((entry['ref'], entry['weights'][term]))\n",
    "        # else create a new key for word\n",
    "        else:\n",
    "            indices[term] = [(entry['ref'], entry['weights'][term])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate through indices and sort the results\n",
    "for term in indices.keys():\n",
    "    indices[term] = sorted(indices[term], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Computed Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pickle to output into a .pkl file for production\n",
    "pickle.dump(indices, open('tf-idf-table.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
